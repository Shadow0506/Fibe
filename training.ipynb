{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed data\n",
    "with open('dataset/preprocessed_data.pkl', 'rb') as f:\n",
    "    preprocessed_data = pickle.load(f)\n",
    "\n",
    "train_df = preprocessed_data['train_df']\n",
    "test_df = preprocessed_data['test_df']\n",
    "le = preprocessed_data['label_encoder']\n",
    "cv_splits = preprocessed_data['cv_splits']\n",
    "\n",
    "# Set up BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IABDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs, checkpoint_path=None):\n",
    "    scaler = GradScaler()\n",
    "    best_accuracy = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_accuracy = checkpoint['best_accuracy']\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader)}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs.logits, dim=1)\n",
    "                predictions.extend(preds.cpu().tolist())\n",
    "                true_labels.extend(labels.cpu().tolist())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = sum([1 for p, t in zip(predictions, true_labels) if p == t]) / len(predictions)\n",
    "        print(f'Validation Loss: {val_loss}, Accuracy: {accuracy}')\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_accuracy': best_accuracy\n",
    "        }, f'checkpoint_epoch_{epoch}.pth')\n",
    "    \n",
    "    return best_accuracy\n",
    "\n",
    "def run_training(num_epochs, batch_size=16, learning_rate=1e-5, model_name='bert-large-uncased'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Use a larger BERT model\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    cv_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "        print(f\"Fold {fold + 1}/{len(cv_splits)}\")\n",
    "        \n",
    "        train_texts = train_df['processed_text'].iloc[train_idx].tolist()\n",
    "        train_labels = train_df['encoded_target'].iloc[train_idx].tolist()\n",
    "        val_texts = train_df['processed_text'].iloc[val_idx].tolist()\n",
    "        val_labels = train_df['encoded_target'].iloc[val_idx].tolist()\n",
    "        \n",
    "        train_dataset = IABDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "        val_dataset = IABDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
    "        model.to(device)\n",
    "\n",
    "        # Use AdamW with weight decay\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        \n",
    "        # Use a warmup scheduler\n",
    "        total_steps = len(train_loader) * num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps)\n",
    "\n",
    "        best_accuracy = train_model(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "        cv_scores.append(best_accuracy)\n",
    "\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean CV score: {np.mean(cv_scores)}\")\n",
    "\n",
    "    # Train on full dataset\n",
    "    full_dataset = IABDataset(train_df['processed_text'].tolist(), train_df['encoded_target'].tolist(), tokenizer, max_length)\n",
    "    full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    total_steps = len(full_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps)\n",
    "\n",
    "    train_model(model, full_loader, val_loader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
    "\n",
    "    # Load best model and predict on test set\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = IABDataset(test_df['processed_text'].tolist(), None, tokenizer, max_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting on test set'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'index': test_df['index'],\n",
    "        'target': le.inverse_transform(predictions)\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "    print(\"Submission file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training with improved parameters\n",
    "run_training(num_epochs=5, batch_size=16, learning_rate=2e-5, model_name='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def predict_on_test_set(model, test_df, tokenizer, le, batch_size=32, max_length=256, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Create test dataset\n",
    "    test_dataset = IABDataset(test_df['processed_text'].tolist(), None, tokenizer, max_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting on test set'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "    # Convert numeric predictions back to original labels\n",
    "    predicted_labels = le.inverse_transform(predictions)\n",
    "\n",
    "    # Create submission DataFrame\n",
    "    if 'Index' in test_df.columns:\n",
    "        index_column = test_df['Index']\n",
    "    elif 'index' in test_df.columns:\n",
    "        index_column = test_df['index']\n",
    "    else:\n",
    "        index_column = [f\"Article_{i}\" for i in test_df.index]\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'target': predicted_labels,\n",
    "        'Index': index_column\n",
    "    })\n",
    "\n",
    "    # Save submission to CSV\n",
    "    submission.to_csv('submission.csv', index=False, quoting=1)  # quoting=1 ensures all fields are quoted\n",
    "    print(\"Submission file created: submission.csv\")\n",
    "\n",
    "    return submission\n",
    "\n",
    "# best_model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=len(le.classes_))\n",
    "# best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "# \n",
    "# submission = predict_on_test_set(best_model, test_df, tokenizer, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your best model\n",
    "best_model = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=len(le.classes_))\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Make predictions\n",
    "submission = predict_on_test_set(best_model, test_df, tokenizer, le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
